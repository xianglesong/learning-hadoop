/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
// File generated by hadoop record compiler. Do not edit.
package org.apache.hadoop.record.test;

import org.apache.hadoop.io.Text;

public class RecRecord1 implements org.apache.hadoop.record.Record, org.apache.hadoop.io.WritableComparable {
  private boolean mBoolVal;
  private byte mByteVal;
  private int mIntVal;
  private long mLongVal;
  private float mFloatVal;
  private double mDoubleVal;
  private Text mStringVal;
  private java.io.ByteArrayOutputStream mBufferVal;
  private java.util.ArrayList mVectorVal;
  private java.util.TreeMap mMapVal;
  private org.apache.hadoop.record.test.RecRecord0 mRecordVal;
  private java.util.BitSet bs_;
  public RecRecord1() {
    bs_ = new java.util.BitSet(12);
    bs_.set(11);
  }
  public RecRecord1(
        boolean m0,
        byte m1,
        int m2,
        long m3,
        float m4,
        double m5,
        Text m6,
        java.io.ByteArrayOutputStream m7,
        java.util.ArrayList m8,
        java.util.TreeMap m9,
        org.apache.hadoop.record.test.RecRecord0 m10) {
    bs_ = new java.util.BitSet(12);
    bs_.set(11);
    mBoolVal=m0; bs_.set(0);
    mByteVal=m1; bs_.set(1);
    mIntVal=m2; bs_.set(2);
    mLongVal=m3; bs_.set(3);
    mFloatVal=m4; bs_.set(4);
    mDoubleVal=m5; bs_.set(5);
    mStringVal=m6; bs_.set(6);
    mBufferVal=m7; bs_.set(7);
    mVectorVal=m8; bs_.set(8);
    mMapVal=m9; bs_.set(9);
    mRecordVal=m10; bs_.set(10);
  }
  public boolean getBoolVal() {
    return mBoolVal;
  }
  public void setBoolVal(boolean m_) {
    mBoolVal=m_; bs_.set(0);
  }
  public byte getByteVal() {
    return mByteVal;
  }
  public void setByteVal(byte m_) {
    mByteVal=m_; bs_.set(1);
  }
  public int getIntVal() {
    return mIntVal;
  }
  public void setIntVal(int m_) {
    mIntVal=m_; bs_.set(2);
  }
  public long getLongVal() {
    return mLongVal;
  }
  public void setLongVal(long m_) {
    mLongVal=m_; bs_.set(3);
  }
  public float getFloatVal() {
    return mFloatVal;
  }
  public void setFloatVal(float m_) {
    mFloatVal=m_; bs_.set(4);
  }
  public double getDoubleVal() {
    return mDoubleVal;
  }
  public void setDoubleVal(double m_) {
    mDoubleVal=m_; bs_.set(5);
  }
  public Text getStringVal() {
    return mStringVal;
  }
  public void setStringVal(Text m_) {
    mStringVal=m_; bs_.set(6);
  }
  public java.io.ByteArrayOutputStream getBufferVal() {
    return mBufferVal;
  }
  public void setBufferVal(java.io.ByteArrayOutputStream m_) {
    mBufferVal=m_; bs_.set(7);
  }
  public java.util.ArrayList getVectorVal() {
    return mVectorVal;
  }
  public void setVectorVal(java.util.ArrayList m_) {
    mVectorVal=m_; bs_.set(8);
  }
  public java.util.TreeMap getMapVal() {
    return mMapVal;
  }
  public void setMapVal(java.util.TreeMap m_) {
    mMapVal=m_; bs_.set(9);
  }
  public org.apache.hadoop.record.test.RecRecord0 getRecordVal() {
    return mRecordVal;
  }
  public void setRecordVal(org.apache.hadoop.record.test.RecRecord0 m_) {
    mRecordVal=m_; bs_.set(10);
  }
  public void serialize(org.apache.hadoop.record.OutputArchive a_, String tag) throws java.io.IOException {
    if (!validate()) throw new java.io.IOException("All fields not set:");
    a_.startRecord(this,tag);
    a_.writeBool(mBoolVal,"BoolVal");
    bs_.clear(0);
    a_.writeByte(mByteVal,"ByteVal");
    bs_.clear(1);
    a_.writeInt(mIntVal,"IntVal");
    bs_.clear(2);
    a_.writeLong(mLongVal,"LongVal");
    bs_.clear(3);
    a_.writeFloat(mFloatVal,"FloatVal");
    bs_.clear(4);
    a_.writeDouble(mDoubleVal,"DoubleVal");
    bs_.clear(5);
    a_.writeString(mStringVal,"StringVal");
    bs_.clear(6);
    a_.writeBuffer(mBufferVal,"BufferVal");
    bs_.clear(7);
    {
      a_.startVector(mVectorVal,"VectorVal");
      int len1 = mVectorVal.size();
      for(int vidx1 = 0; vidx1<len1; vidx1++) {
        Text e1 = (Text) mVectorVal.get(vidx1);
        a_.writeString(e1,"e1");
      }
      a_.endVector(mVectorVal,"VectorVal");
    }
    bs_.clear(8);
    {
      a_.startMap(mMapVal,"MapVal");
      java.util.Set es1 = mMapVal.entrySet();
      for(java.util.Iterator midx1 = es1.iterator(); midx1.hasNext(); ) {
        java.util.Map.Entry me1 = (java.util.Map.Entry) midx1.next();
        Text k1 = (Text) me1.getKey();
        Text v1 = (Text) me1.getValue();
        a_.writeString(k1,"k1");
        a_.writeString(v1,"v1");
      }
      a_.endMap(mMapVal,"MapVal");
    }
    bs_.clear(9);
    a_.writeRecord(mRecordVal,"RecordVal");
    bs_.clear(10);
    a_.endRecord(this,tag);
  }
  public void deserialize(org.apache.hadoop.record.InputArchive a_, String tag) throws java.io.IOException {
    a_.startRecord(tag);
    mBoolVal=a_.readBool("BoolVal");
    bs_.set(0);
    mByteVal=a_.readByte("ByteVal");
    bs_.set(1);
    mIntVal=a_.readInt("IntVal");
    bs_.set(2);
    mLongVal=a_.readLong("LongVal");
    bs_.set(3);
    mFloatVal=a_.readFloat("FloatVal");
    bs_.set(4);
    mDoubleVal=a_.readDouble("DoubleVal");
    bs_.set(5);
    mStringVal=a_.readString("StringVal");
    bs_.set(6);
    mBufferVal=a_.readBuffer("BufferVal");
    bs_.set(7);
    {
      org.apache.hadoop.record.Index vidx1 = a_.startVector("VectorVal");
      mVectorVal=new java.util.ArrayList();
      for (; !vidx1.done(); vidx1.incr()) {
    Text e1;
        e1=a_.readString("e1");
        mVectorVal.add(e1);
      }
    a_.endVector("VectorVal");
    }
    bs_.set(8);
    {
      org.apache.hadoop.record.Index midx1 = a_.startMap("MapVal");
      mMapVal=new java.util.TreeMap();
      for (; !midx1.done(); midx1.incr()) {
    Text k1;
        k1=a_.readString("k1");
    Text v1;
        v1=a_.readString("v1");
        mMapVal.put(k1,v1);
      }
    a_.endMap("MapVal");
    }
    bs_.set(9);
    mRecordVal= new org.apache.hadoop.record.test.RecRecord0();
    a_.readRecord(mRecordVal,"RecordVal");
    bs_.set(10);
    a_.endRecord(tag);
}
  public String toString() {
    try {
      java.io.ByteArrayOutputStream s =
        new java.io.ByteArrayOutputStream();
      org.apache.hadoop.record.CsvOutputArchive a_ = 
        new org.apache.hadoop.record.CsvOutputArchive(s);
      a_.startRecord(this,"");
    a_.writeBool(mBoolVal,"BoolVal");
    a_.writeByte(mByteVal,"ByteVal");
    a_.writeInt(mIntVal,"IntVal");
    a_.writeLong(mLongVal,"LongVal");
    a_.writeFloat(mFloatVal,"FloatVal");
    a_.writeDouble(mDoubleVal,"DoubleVal");
    a_.writeString(mStringVal,"StringVal");
    a_.writeBuffer(mBufferVal,"BufferVal");
    {
      a_.startVector(mVectorVal,"VectorVal");
      int len1 = mVectorVal.size();
      for(int vidx1 = 0; vidx1<len1; vidx1++) {
        Text e1 = (Text) mVectorVal.get(vidx1);
        a_.writeString(e1,"e1");
      }
      a_.endVector(mVectorVal,"VectorVal");
    }
    {
      a_.startMap(mMapVal,"MapVal");
      java.util.Set es1 = mMapVal.entrySet();
      for(java.util.Iterator midx1 = es1.iterator(); midx1.hasNext(); ) {
        java.util.Map.Entry me1 = (java.util.Map.Entry) midx1.next();
        Text k1 = (Text) me1.getKey();
        Text v1 = (Text) me1.getValue();
        a_.writeString(k1,"k1");
        a_.writeString(v1,"v1");
      }
      a_.endMap(mMapVal,"MapVal");
    }
    a_.writeRecord(mRecordVal,"RecordVal");
      a_.endRecord(this,"");
      return new String(s.toByteArray(), "UTF-8");
    } catch (Throwable ex) {
      ex.printStackTrace();
    }
    return "ERROR";
  }
  public void write(java.io.DataOutput out) throws java.io.IOException {
    org.apache.hadoop.record.BinaryOutputArchive archive = new org.apache.hadoop.record.BinaryOutputArchive(out);
    serialize(archive, "");
  }
  public void readFields(java.io.DataInput in) throws java.io.IOException {
    org.apache.hadoop.record.BinaryInputArchive archive = new org.apache.hadoop.record.BinaryInputArchive(in);
    deserialize(archive, "");
  }
  public boolean validate() {
    if (bs_.cardinality() != bs_.length()) return false;
    if (!mRecordVal.validate()) return false;
    return true;
}
  public int compareTo (Object peer_) throws ClassCastException {
    if (!(peer_ instanceof RecRecord1)) {
      throw new ClassCastException("Comparing different types of records.");
    }
    RecRecord1 peer = (RecRecord1) peer_;
    int ret = 0;
    ret = (mBoolVal == peer.mBoolVal)? 0 : (mBoolVal?1:-1);
    if (ret != 0) return ret;
    ret = (mByteVal == peer.mByteVal)? 0 :((mByteVal<peer.mByteVal)?-1:1);
    if (ret != 0) return ret;
    ret = (mIntVal == peer.mIntVal)? 0 :((mIntVal<peer.mIntVal)?-1:1);
    if (ret != 0) return ret;
    ret = (mLongVal == peer.mLongVal)? 0 :((mLongVal<peer.mLongVal)?-1:1);
    if (ret != 0) return ret;
    ret = (mFloatVal == peer.mFloatVal)? 0 :((mFloatVal<peer.mFloatVal)?-1:1);
    if (ret != 0) return ret;
    ret = (mDoubleVal == peer.mDoubleVal)? 0 :((mDoubleVal<peer.mDoubleVal)?-1:1);
    if (ret != 0) return ret;
    ret = mStringVal.compareTo(peer.mStringVal);
    if (ret != 0) return ret;
    if (ret != 0) return ret;
    if (ret != 0) return ret;
    if (ret != 0) return ret;
    ret = mRecordVal.compareTo(peer.mRecordVal);
    if (ret != 0) return ret;
     return ret;
  }
  public boolean equals(Object peer_) {
    if (!(peer_ instanceof RecRecord1)) {
      return false;
    }
    if (peer_ == this) {
      return true;
    }
    RecRecord1 peer = (RecRecord1) peer_;
    boolean ret = false;
    ret = (mBoolVal==peer.mBoolVal);
    if (!ret) return ret;
    ret = (mByteVal==peer.mByteVal);
    if (!ret) return ret;
    ret = (mIntVal==peer.mIntVal);
    if (!ret) return ret;
    ret = (mLongVal==peer.mLongVal);
    if (!ret) return ret;
    ret = (mFloatVal==peer.mFloatVal);
    if (!ret) return ret;
    ret = (mDoubleVal==peer.mDoubleVal);
    if (!ret) return ret;
    ret = mStringVal.equals(peer.mStringVal);
    if (!ret) return ret;
    ret = org.apache.hadoop.record.Utils.bufEquals(mBufferVal,peer.mBufferVal);
    if (!ret) return ret;
    ret = mVectorVal.equals(peer.mVectorVal);
    if (!ret) return ret;
    ret = mMapVal.equals(peer.mMapVal);
    if (!ret) return ret;
    ret = mRecordVal.equals(peer.mRecordVal);
    if (!ret) return ret;
     return ret;
  }
  public int hashCode() {
    int result = 17;
    int ret;
     ret = (mBoolVal)?0:1;
    result = 37*result + ret;
    ret = (int)mByteVal;
    result = 37*result + ret;
    ret = (int)mIntVal;
    result = 37*result + ret;
    ret = (int) (mLongVal^(mLongVal>>>32));
    result = 37*result + ret;
    ret = Float.floatToIntBits(mFloatVal);
    result = 37*result + ret;
    ret = (int)(Double.doubleToLongBits(mDoubleVal)^(Double.doubleToLongBits(mDoubleVal)>>>32));
    result = 37*result + ret;
    ret = mStringVal.hashCode();
    result = 37*result + ret;
    ret = mBufferVal.toString().hashCode();
    result = 37*result + ret;
    ret = mVectorVal.hashCode();
    result = 37*result + ret;
    ret = mMapVal.hashCode();
    result = 37*result + ret;
    ret = mRecordVal.hashCode();
    result = 37*result + ret;
    return result;
  }
  public static String signature() {
    return "LRecRecord1(zbilfdsB[s]{ss}LRecRecord0(s))";
  }
}
